# -*- coding: utf-8 -*-
"""MLEx9.Regularization_in_ANNs

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TkQ9B_eRwVE8j5zaVeMcxSrvdi_LHXbU

# Machine Learning - Exercise 9
# Regularization in ANNs

This exercise illustrates the effects of applying regularization in ANNs.

##Import 

Import libraries that contains the implementations of the functions used in the rest of the program.
"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
import tensorflow as tf
import tensorflow.keras as keras

import random
import numpy as np
import sklearn.metrics 
from sklearn import datasets
from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, GridSearchCV
from sklearn import svm
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils.multiclass import unique_labels
import matplotlib.pyplot as plt

# fix random seed
np.random.seed(123)

def plot_confusion_matrix(y_true, y_pred, classes,
                          normalize=False,
                          title=None,
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if not title:
        if normalize:
            title = 'Normalized confusion matrix'
        else:
            title = 'Confusion matrix, without normalization'

    # Compute confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    # Only use the labels that appear in the data
    classes = classes[unique_labels(y_true, y_pred)]
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        #print("Normalized confusion matrix")
    else:
        pass
        #print('Confusion matrix, without normalization')

    #print(cm)

    fig, ax = plt.subplots()
    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)
    ax.figure.colorbar(im, ax=ax)
    # We want to show all ticks...
    ax.set(xticks=np.arange(cm.shape[1]),
           yticks=np.arange(cm.shape[0]),
           # ... and label them with the respective list entries
           xticklabels=classes, yticklabels=classes,
           title=title,
           ylabel='True label',
           xlabel='Predicted label')

    # Rotate the tick labels and set their alignment.
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
             rotation_mode="anchor")

    # Loop over data dimensions and create text annotations.
    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j, i, format(cm[i, j], fmt),
                    ha="center", va="center",
                    color="white" if cm[i, j] > thresh else "black")
    fig.tight_layout()
    return ax

print("Libraries imported.")

"""## Load data

Load the dataset.

More details on the MNIST dataset are available in http://yann.lecun.com/exdb/mnist/

More details on the Fashion-MNIST dataset are available in https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/

### MNIST dataset
"""

DB = keras.datasets.mnist
dataset_name = 'MNIST'

"""### Fashion-MNIST dataset"""

DB = keras.datasets.fashion_mnist
dataset_name = 'FASHION_MNIST'

"""### Load data

Data are normalized in [0,1]
"""

(X_train, y_train), (X_test, y_test) = DB.load_data()
X_train, X_test = X_train / 255.0, X_test / 255.0

"""###Summary of the dataset

Print some information about the dataset
"""

class_names = np.array(range(10))
print(X_train.shape)
print(y_train.shape)

ninput = X_train.shape[1]
nsamples = X_train.shape[0]

print("Dataset: %s" %(dataset_name))
print("Number of attributes/features: %d" %(ninput))
print("Number of classes: %d %s" %(len(class_names), str(class_names)))
print("Number of samples: %d" %(nsamples))

"""### Show an example

Display a random sample.
"""

id = random.randrange(0,X_train.shape[0])
plt.imshow(X_train[id],cmap='gray')
plt.title('Label: {}'.format(y_train[id]))
plt.show()

"""##Create ANN Models

Model fitting = finding a solution that is stored in the model.

IMPORTANT: solution depends on data split

**1st model** - No regularization (baseline)
"""

models = []
titles = ['Baseline', 'Weight Decay', 'Dropout']

model_tmp = keras.models.Sequential([
  keras.layers.Flatten(input_shape=(28, 28)),
  keras.layers.Dense(64, activation='relu'),
  keras.layers.Dense(32, activation='relu'),
  keras.layers.Dense(len(class_names), activation='softmax')
])
Wsave = model_tmp.get_weights() # save the initialized weights 
print(model_tmp.summary())
models.append(model_tmp)

"""**2nd model** - Weight decay"""

model_tmp = keras.models.Sequential([
  keras.layers.Flatten(input_shape=(28, 28)),
  keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),
  keras.layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),
  keras.layers.Dense(len(class_names), activation='softmax')
])
model_tmp.set_weights(Wsave) # use the same initialized weights
print(model_tmp.summary())
models.append(model_tmp)

"""**3rd model** - Dropout"""

model_tmp = keras.models.Sequential([
  keras.layers.Flatten(input_shape=(28, 28)),
  keras.layers.Dense(64, activation='relu'),
  keras.layers.Dense(32, activation='relu'),
  keras.layers.Dropout(0.2),
  keras.layers.Dense(len(class_names), activation='softmax')
])
model_tmp.set_weights(Wsave) # use the same initialized weights
print(model_tmp.summary())
models.append(model_tmp)

"""### Define optimizer and compile the models

Note:  
loss `categorical_crossentropy` used wjen classes encoded with one_hot representation

loss `sparse_categorical_crossentropy` is used when classes encoded with integer numbers
"""

optimizer = "adam" # choose between adam, adadelta, sgd

if optimizer == "adam":
  pass
elif optimizer == "adadelta":
  pass
elif optimizer == "sgd":
  # in this case set parameters of the optimizer instead of leaving the default ones
  learning_rate = 0.01
  momentum = 1.0
  optimizer = keras.optimizers.SGD(lr = learning_rate, momentum = momentum)

for m in models:
  m.compile(optimizer=optimizer,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

"""## Fit the model

Using validation data
"""

nepochs = 50
batch_size = 128

histories = []
for i,m in enumerate(models):
  print('Training {} model'.format(titles[i]))
  histories.append(m.fit(X_train, y_train, batch_size=batch_size, epochs=nepochs, validation_data=(X_test,y_test)))

"""## Loss and Accuracy Plots

### Loss comparison
"""

fig=plt.figure(figsize=(16, 8))
for i,h in enumerate(histories):
  plt.subplot(1,3,i+1)
  plt.plot(h.history['loss'],'r')
  plt.plot(h.history['val_loss'],'b')
  plt.ylim((0.,1.))
  plt.title(titles[i])
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
plt.suptitle('Model loss comparison', fontsize=14)
plt.show()

fig=plt.figure(figsize=(8, 8))
for h in histories:
  plt.plot(h.history['val_loss'])
  plt.ylim((0.,1.))
  plt.title('Model Loss Comparison on Test')
  plt.ylabel('loss')
  plt.xlabel('epoch')
  plt.legend(titles, loc='upper left')
plt.show()

"""### Accuracy comparison"""

fig=plt.figure(figsize=(16, 8))
for i,h in enumerate(histories):
  plt.subplot(1,3,i+1)
  plt.plot(h.history['accuracy'],'r')
  plt.plot(h.history['val_accuracy'],'b')
  plt.ylim((0.75,1.))
  plt.title(titles[i])
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(['train', 'test'], loc='upper left')
plt.suptitle('Model accuracy comparison', fontsize=14)
plt.show()

fig=plt.figure(figsize=(8, 8))
for h in histories:
  plt.plot(h.history['val_accuracy'])
  plt.ylim((0.8,1.))
  plt.title('Model Accuracy Comparison on Test')
  plt.ylabel('accuracy')
  plt.xlabel('epoch')
  plt.legend(titles, loc='upper left')
plt.show()

"""##Predict on test set

Use test set for prediction.
"""

model = models[2]
y_pred_sm = model.predict(X_test)
y_pred = np.argmax(y_pred_sm, axis=1)

"""##Evaluate the Model

Evaluation of a specific solution.

### Accuracy
"""

acc = model.evaluate(X_test, y_test)
print("Accuracy %.3f" %acc[-1])

"""###Precision & Recall"""

print(classification_report(y_test, y_pred, labels=None, digits=3))

"""### Confusion Matrix"""

cm = confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)
print(cm)
plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=False)

"""# Home Exercises

**Question 1**

Try two more configurations for regulation and compare them with the best result obtained in the above tests.
"""