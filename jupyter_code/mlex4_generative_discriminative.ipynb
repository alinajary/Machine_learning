# -*- coding: utf-8 -*-
"""MLEx4.Generative_Discriminative

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AviuuDM-DoUB9UedKRHWq_soL9m0RGU-

# Machine Learning - Exercise 4
# Generative - Discriminative Classifiers

##Import 

Import libraries that contains the implementations of the functions used in the rest of the program.
"""

import random
import numpy as np
import sklearn.metrics 
from sklearn.metrics import accuracy_score
from sklearn import datasets
from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, GridSearchCV
from sklearn import svm
from sklearn import tree
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils.multiclass import unique_labels
import matplotlib.pyplot as plt


print("Libraries imported.")

"""# Data sets

## Load data

Load training data.

Choose one of the datasets described in the next subsections.

More details on these datasets are available in https://scikit-learn.org/stable/datasets/index.html

"""

dataset_name = "Iris" # "Iris", "Wines"

if dataset_name == "Iris":
  DB = datasets.load_iris()
  class_names = np.array([str(c) for c in DB.target_names])
  X_all = DB.data
  y_all = DB.target

elif dataset_name == "Wines":
  DB = datasets.load_wine()
  class_names = np.array([str(c) for c in DB.target_names])
  X_all = DB.data
  y_all = DB.target


# print specs
print("Dataset: %s" %(dataset_name))
print("Input shape: %s" %str(X_all.shape))
print("Output shape: %s" %str(y_all.shape))
print("Number of attributes/features: %d" %(X_all.shape[1]))
print("Number of classes: %d %s" %(len(class_names), str(class_names)))
print("Number of samples: %d" %(X_all.shape[0]))

# show an example
id = random.randrange(0,X_all.shape[0])

print("Example:")
print("x%d = %r" %(id,X_all[id]))
print("y%d = %r ['%s']" %(id,y_all[id],class_names[y_all[id]]))

"""##Split data

Split the data set in training set `X_train`, `y_train` and test set `X_test`, `y_test`, using `test_size` to denote percentage of samples in the test set.

Note:
`random_state` parameter is used as random seed. Change this value to generate a different split.

IMPORTANT: solution will depend on this data split.

"""

X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.333, 
                                                    random_state=117)

print("Size of training set: %d" %X_train.shape[0])
print("Size of test set: %d" %X_test.shape[0])

print('First training sample')
id = 0
print("    x_train_%d = %r" %(id,X_train[id]))
print("    y_train_%d = %r ['%s']" %(id,y_train[id],class_names[y_train[id]]))

print('First test sample')
id = 0
print("    x_test_%d = %r" %(id,X_test[id]))
print("    y_test_%d = %r ['%s']" %(id,y_test[id],class_names[y_test[id]]))

"""## Reduce data set to two dimensions

Reduce feature dimensions to 2. We consider only a 2D input for visualizing the data generation capability of this model using 2D plots.
"""

# select the first two features
X_train_reduced = X_train[:,0:2]
X_test_reduced = X_test[:,0:2]
print("Input shape reduced train: %s" %str(X_train_reduced.shape))
print("Input shape reduced test: %s" %str(X_test_reduced.shape))

"""## Visualize data

Visualize reduced training/test sets
"""

plt.figure()
plt.scatter(X_train_reduced[:,0], X_train_reduced[:,1], c=y_train)
plt.scatter(X_test_reduced[:,0], X_test_reduced[:,1], c=y_test, marker="*")
plt.grid(linestyle="--")
plt.title("Training and test samples")

"""# Generative model"""

gen_model = GaussianNB()

"""Fit generative model on reduced training data and print generative parameters:

> Means and standard deviations of fitted Gaussians are of dimension *(num_classes x num_features)*

"""

gen_model.fit(X_train_reduced, y_train)
print("Means:", gen_model.theta_)
print("Means shape: ", gen_model.theta_.shape)
print("\n")
print("Standard deviations:", gen_model.sigma_)
print("Standard deviations shape: ", gen_model.sigma_.shape)

"""## Visualize generated data by generative model

Sample learnt Gaussian distributions and visualize them
"""

def plot_ellipse(ax, mu, sigma, color="k", label=None):
    """
    Based on
    http://stackoverflow.com/questions/17952171/not-sure-how-to-fit-data-with-a-gaussian-python.
    """
    from matplotlib.patches import Ellipse
    # Compute eigenvalues and associated eigenvectors
    vals, vecs = np.linalg.eigh(sigma)

    # Compute "tilt" of ellipse using first eigenvector
    x, y = vecs[:, 0]
    theta = np.degrees(np.arctan2(y, x))

    # Eigenvalues give length of ellipse along each eigenvector
    w, h = 2 * np.sqrt(vals)

    ax.tick_params(axis='both', which='major', labelsize=20)
    ellipse = Ellipse(mu, w, h, theta, color=color, label=label)  # color="k")
    ellipse.set_clip_box(ax.bbox)
    ellipse.set_alpha(0.1)
    ax.add_artist(ellipse)
    return ellipse

n_sampled_points = 300
sampled_data = []
sampled_labels = []

plt.figure()
for class_id in range(len(class_names)):

  # plot ellipse of gaussian distribution
  plot_ellipse(plt.gca(), gen_model.theta_[class_id], np.identity(2)*gen_model.sigma_[class_id])

  x_sampled = np.random.multivariate_normal(gen_model.theta_[class_id], np.identity(2)*gen_model.sigma_[class_id], n_sampled_points)
  y_sampled = np.repeat(class_id, repeats=n_sampled_points)
  sampled_data.append(x_sampled)
  sampled_labels.append(y_sampled)

sampled_data = np.concatenate(sampled_data,axis=0)
sampled_labels = np.concatenate(sampled_labels,axis=0)

# plot generated data
plt.scatter(sampled_data[:,0], sampled_data[:,1], c=sampled_labels, alpha=0.1)

# plot training data
plt.scatter(X_train_reduced[:,0], X_train_reduced[:,1], c=y_train)

plt.xlim(np.min(X_train_reduced[:,0]), np.max(X_train_reduced[:,0]))
plt.ylim(np.min(X_train_reduced[:,1]), np.max(X_train_reduced[:,1]))
plt.grid(linestyle="--")
plt.title("Training and generated samples")

"""Evaluate generative model in classification task"""

y_pred = gen_model.predict(X_test_reduced)
acc = accuracy_score(y_pred, y_test)    
print("Accuracy %.3f" %acc)

"""## Test generative model resistance to label noise

Noisy labels
"""

# randomly permute a percentage of training labels
percentage = 0.4

y_train_noisy = np.copy(y_train)

ix_size = int(percentage * len(y_train_noisy))
ix = np.random.choice(len(y_train_noisy), size=ix_size, replace=False)
b = y_train[ix]
np.random.shuffle(b)
y_train_noisy[ix] = b

# plot original training data
plt.figure()
plt.scatter(X_train_reduced[:,0], X_train_reduced[:,1], c=y_train)
plt.grid(linestyle="--")
plt.title("Original training samples")

# plot noisy training data

plt.figure()
plt.scatter(X_train_reduced[:,0], X_train_reduced[:,1], c=y_train_noisy)
plt.grid(linestyle="--")
plt.title("Noisy training samples")

"""Fit model to noisy labels"""

gen_model = GaussianNB()
gen_model.fit(X_train_reduced, y_train_noisy)
print("Means:", gen_model.theta_)
print("Means shape: ", gen_model.theta_.shape)
print("\n")
print("Standard deviations:", gen_model.sigma_)
print("Standard deviations shape: ", gen_model.sigma_.shape)

"""Visualize generated samples from noisy estimates"""

n_sampled_points = 300
sampled_data = []
sampled_labels = []

plt.figure()
for class_id in range(len(class_names)):

  # plot ellipse of gaussian distribution
  plot_ellipse(plt.gca(), gen_model.theta_[class_id], np.identity(2)*gen_model.sigma_[class_id])

  x_sampled = np.random.multivariate_normal(gen_model.theta_[class_id], np.identity(2)*gen_model.sigma_[class_id], n_sampled_points)
  y_sampled = np.repeat(class_id, repeats=n_sampled_points)
  sampled_data.append(x_sampled)
  sampled_labels.append(y_sampled)

sampled_data = np.concatenate(sampled_data,axis=0)
sampled_labels = np.concatenate(sampled_labels,axis=0)

# plot generated data
plt.scatter(sampled_data[:,0], sampled_data[:,1], c=sampled_labels, alpha=0.1)

# plot original training data
plt.scatter(X_train_reduced[:,0], X_train_reduced[:,1], c=y_train)

plt.xlim(np.min(X_train_reduced[:,0]), np.max(X_train_reduced[:,0]))
plt.ylim(np.min(X_train_reduced[:,1]), np.max(X_train_reduced[:,1]))
plt.grid(linestyle="--")
plt.title("Original training and noisy generated samples")

"""Evaluate generative model in classification task with noisy labels"""

y_pred = gen_model.predict(X_test_reduced)
acc = accuracy_score(y_pred, y_test)    
print("Accuracy %.3f" %acc)

"""# Discriminative model"""

discrim_model = LogisticRegression()

"""## Visualize decision boundaries of discriminant classifier

Reduce feature dimensions to 2. We consider only a 2D input for visualizing the decision boundaries of the classifier using 2D plots.
"""

# select the first two features
X_train_reduced = X_train[:,0:2]
X_test_reduced = X_test[:,0:2]
print("Input shape reduced train: %s" %str(X_train_reduced.shape))
print("Input shape reduced test: %s" %str(X_test_reduced.shape))

"""Fit classifier on data"""

discrim_model.fit(X_train_reduced, y_train)

"""Plot decision boundaries of each class versus other classes"""

xx, yy = np.mgrid[np.min(X_train_reduced[:,0]):np.max(X_train_reduced[:,0]):.01, np.min(X_train_reduced[:,1]):np.max(X_train_reduced[:,1]):.01]
grid = np.c_[xx.ravel(), yy.ravel()]

for class_id in range(len(class_names)):
  # compute probability to belong to class class_id
  probs = discrim_model.predict_proba(grid)[:, class_id].reshape(xx.shape)

  f, ax = plt.subplots(figsize=(8, 6))
  contour = ax.contourf(xx, yy, probs, 25, cmap="RdBu", vmin=0, vmax=1)
  ax_c = f.colorbar(contour)
  ax_c.set_label("$P(y = k)$")
  ax_c.set_ticks([0, .25, .5, .75, 1])

  mask = y_train == class_id

  ax.scatter(X_train_reduced[mask,0], X_train_reduced[mask,1], c="b", s=50, vmin=-.2, vmax=1.2, edgecolor="w", linewidth=1)
  ax.scatter(X_train_reduced[np.logical_not(mask),0], X_train_reduced[np.logical_not(mask),1], c="r", s=50, vmin=-.2, vmax=1.2, edgecolor="w", linewidth=1)

  ax.set(aspect="equal",
        xlim=(np.min(X_train_reduced[:,0]), np.max(X_train_reduced[:,0])), ylim=(np.min(X_train_reduced[:,1]), np.max(X_train_reduced[:,1])),
        xlabel="$X_1$", ylabel="$X_2$")
  ax.set_title("original points of class {} (in blue) vs the rest (in red)".format(class_id))

"""Evaluate discriminative model on classification task"""

y_pred = discrim_model.predict(X_test_reduced)
acc = accuracy_score(y_pred, y_test)    
print("Accuracy %.3f" %acc)

"""##Test discriminative model resistance to label noise

Noisy labels
"""

# randomly permute a percentage of training labels
percentage = 0.4

y_train_noisy = np.copy(y_train)

ix_size = int(percentage * len(y_train_noisy))
ix = np.random.choice(len(y_train_noisy), size=ix_size, replace=False)
b = y_train[ix]
np.random.shuffle(b)
y_train_noisy[ix] = b

# plot original training data
plt.figure()
plt.scatter(X_train_reduced[:,0], X_train_reduced[:,1], c=y_train)
plt.grid(linestyle="--")
plt.title("Original training samples")

# plot noisy training data
plt.figure()
plt.scatter(X_train_reduced[:,0], X_train_reduced[:,1], c=y_train_noisy)
plt.grid(linestyle="--")
plt.title("Noisy training samples")

"""Fit model to noisy labels"""

discrim_model = LogisticRegression()
discrim_model.fit(X_train_reduced, y_train_noisy)

"""Visualize noisy decision boundaries"""

xx, yy = np.mgrid[np.min(X_train_reduced[:,0]):np.max(X_train_reduced[:,0]):.01, np.min(X_train_reduced[:,1]):np.max(X_train_reduced[:,1]):.01]
grid = np.c_[xx.ravel(), yy.ravel()]

for class_id in range(len(class_names)):
  # compute probability to belong to class class_id
  probs = discrim_model.predict_proba(grid)[:, class_id].reshape(xx.shape)

  f, ax = plt.subplots(figsize=(8, 6))
  contour = ax.contourf(xx, yy, probs, 25, cmap="RdBu", vmin=0, vmax=1)
  ax_c = f.colorbar(contour)
  ax_c.set_label("$P(y = k)$")
  ax_c.set_ticks([0, .25, .5, .75, 1])

  mask = y_train == class_id

  ax.scatter(X_train_reduced[mask,0], X_train_reduced[mask,1], c="b", s=50, vmin=-.2, vmax=1.2, edgecolor="w", linewidth=1)
  ax.scatter(X_train_reduced[np.logical_not(mask),0], X_train_reduced[np.logical_not(mask),1], c="r", s=50, vmin=-.2, vmax=1.2, edgecolor="w", linewidth=1)

  ax.set(aspect="equal",
        xlim=(np.min(X_train_reduced[:,0]), np.max(X_train_reduced[:,0])), ylim=(np.min(X_train_reduced[:,1]), np.max(X_train_reduced[:,1])),
        xlabel="$X_1$", ylabel="$X_2$")
  ax.set_title("points of class {} (in blue) vs the rest (in red)".format(class_id))

"""Evaluate discriminative model on classification task"""

y_pred = discrim_model.predict(X_test_reduced)
acc = accuracy_score(y_pred, y_test)    
print("Accuracy %.3f" %acc)

"""# Home Exercises

Consider all the datasets proposed in this exercise with all input features.

**Question 1**

Compare the performance of both models considering different percentages of noisy labels. Plot performace (e.g., accuracy) over percentage of noisy labels,condsidering percentage values 0.0, 0.25, 0.5, 0.75, 1.

**Question 2** 

Compare the performance of both models considering different percentages of random samples, i.e., samples randomly gathered from random distributions and assigned to each class. 
Plot performace (e.g., accuracy) over percentage of random samples, condsidering percentage values 0.0, 0.25, 0.5, 0.75, 1.

"""