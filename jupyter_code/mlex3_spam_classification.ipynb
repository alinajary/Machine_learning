# -*- coding: utf-8 -*-
"""MLEx3.SPAM_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ey_tVYreriubDr8KAjzQABRmqOs2sV96

# Machine Learning - Exercise 3
# SMS SPAM classification

To perform the experiments on the SMSSpamCollection dataset you need to set-up your Colab such that it is able to load the desired data. To achieve this, you need to perform the following actions:

*   download the dataset available at this [link](https://drive.google.com/a/diag.uniroma1.it/file/d/17YZemn1MidhFA0-wenfVolZAwclLRUXM/view)
*   copy the dataset in a folder of your personal Drive
*   mount your Google Drive (more details will follow)
*   set the correct path for loading the dataset (more details will follow)

## Import needed libraries
"""

import numpy as np
import pandas as pd
import random

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import *
from sklearn.naive_bayes import *
from sklearn.metrics import confusion_matrix, classification_report

print('Libraries imported.')

"""## Load data

Mount Google Drive by following the instructions given at the provided link
"""

from google.colab import drive
drive.mount('/content/drive')

"""To load the file set the correct path of the dataset located in your drive. Once mounted, your drive works like a Linux system, so you can check folders etc... running commands like `ls` or `cd` preceded by `%`"""

# Commented out IPython magic to ensure Python compatibility.
# %ls

# example path of dataset copied in My Drive folder: /content/drive/My Drive/SMSSpamCollection'
filename = '/content/drive/Shared drives/Machine Learning/Datasets/SMSSpamCollection'
db = pd.read_csv(filename, sep='\t', header=None, names=['label', 'text'])
print('File loaded: %d samples.' %(len(db.label)))

"""Show a random sample"""

id = random.randrange(0,len(db.label))
print('%d %s %s' %(id,db.label[id],db.text[id]))

"""## Choose vectorizer

Compute vectorizer terms for all messages. More info:



*   [Hashing](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html)
*   [Count](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)
*   [Tfid](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) 


"""

vectorizer_type = "hashing" # "hashing", "count" or "tfid"

if vectorizer_type == "hashing":
  vectorizer = HashingVectorizer(stop_words='english') # multivariate
elif vectorizer_type == "count":
  vectorizer = CountVectorizer(stop_words='english') # multinomial
elif vectorizer_type == "tfid":
  vectorizer = TfidfVectorizer(stop_words='english')

X_all = vectorizer.fit_transform(db.text)
y_all = db.label

print(X_all.shape)
print(y_all.shape)

"""## Split data"""

X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, 
          test_size=0.2, random_state=15)

id = random.randrange(0,X_train.shape[0])
print('%d ' %(id))

print("Train: %d - Test: %d" %(X_train.shape[0],X_test.shape[0]))
#print('%d %s %s' %(id,str(y_train[id]),str(X_train[id])))

"""## Create and fit Model"""

model_type = "bernoulli" # "bernoulli" or "multinomial"

if model_type == "bernoulli":
  model = BernoulliNB().fit(X_train, y_train)
  print('Bernoulli Model created')
elif model_type == "multinomial":
  model = MultinomialNB().fit(X_train, y_train)
  print('Multinomial Model created')

"""## Evaluation"""

y_pred = model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""## Prediction"""

smsnew1 = np.array(['Hello, did you solve ML exercise?'])
xnew1 = vectorizer.transform(smsnew1)
ynew1 = model.predict(xnew1)
print('%s %s' %(smsnew1,ynew1))

smsnew2 = np.array(['You just won $1,000! Call now 18001234567'])
xnew2 = vectorizer.transform(smsnew2)
ynew2 = model.predict(xnew2)
print('%s %s' %(smsnew2,ynew2))

"""## Home Exercises

**Question 1**

Design and implement an evaluation procedure to assess and compare the performance of the three vectorizers and the two models proposed above.



"""