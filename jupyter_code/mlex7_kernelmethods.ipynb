# -*- coding: utf-8 -*-
"""MLEx7.KernelMethods

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z4hFhv9ql0OFNrpYmJBTP-uGFGSRAO5p

# Machine Learning - Exercise 7
# Kernel Methods

##Import 

Import libraries that contains the implementations of the functions used in the rest of the program.
"""

import random
import numpy as np
from sklearn import svm
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.metrics import mean_squared_error, r2_score



print("Libraries imported.")

"""# Classification data set

Data-generation function
"""

# Param: n=size of data set, outliers=True/False
def generateData(n, outliers=False):
    X = np.ndarray((n,2))
    t = np.ndarray((n,1))
    n1 = int(n*0.5)

    # define random centers of distributions far away
    
    for i in range(0,n1):
        X[i,:] = np.random.normal((2.0,8.5),0.5,size=(1,2))
        t[i] = -1
    for i in range(n1,n):
        X[i,:] = np.random.normal((4.0,5.0),0.3,size=(1,2))
        t[i] = 1
    
    if (outliers):
        no=int(n*0.9)
        for i in range (no,n):
            X[i,:] = np.random.normal((9.0,3.0),0.2,size=(1,2))
            t[i] = 1

    return [X,t]

"""Generate n data points with/without outliers"""

n = 100
outliers = True
np.random.seed(123)

X, t = generateData(n, outliers=outliers)

# print specs
print("Input shape: %s" %str(X.shape))
print("Output shape: %s" %str(t.shape))
print("Number of attributes/features: %d" %(X.shape[1]))
print("Number of classes: %d" %(len(np.unique(t))))
print("Number of samples: %d" %(X.shape[0]))

# show an example
id = random.randrange(0,X.shape[0])

print("Example:")
print("x%d = %r" %(id,X[id]))
print("y%d = %r" %(id,t[id]))

"""# Classification model

Choose among different kernel functions for the SVM classifier [info](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC):
* Linear
* Polynomial
* Radial Basis Function (RBF)
* Sigmoid
"""

kernel_name  = 'poly' # 'linear', 'poly', 'rbf', 'sigmoid'

classifier = svm.SVC(C=1.0, kernel=kernel_name, degree=3, gamma='scale')

"""Fit classifier"""

# train the classifier
classifier.fit(X,t)

"""# Visualize results

Define visualization functions
"""

# Plot data
def plotData(X,t,XNr=None,XNg=None,label='Data'):
    Xr = np.ndarray((0,2))
    Xg = np.ndarray((0,2))
    for i in range(0,len(X)):
        if (t[i][0]==1):
            Xr = np.vstack([Xr, [X[i,0],X[i,1]]])
        else:
            Xg = np.vstack([Xg, [X[i,0],X[i,1]]])
            
    if (not XNr is None):
        plt.plot(XNr[:,0],XNr[:,1],'.', color='#FFAAAA')
    if (not XNg is None):
        plt.plot(XNg[:,0],XNg[:,1],'.', color='#AAFFAA')

    plt.plot(Xr[:,0],Xr[:,1],'ro')
    plt.plot(Xg[:,0],Xg[:,1],'go')
    plt.axis([0,10,0,10])
    plt.title(label)
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.show()
    
# Plot results of a solution
def plotResult(X, t, clf, label):
    XNr = np.ndarray((0,2))
    XNg = np.ndarray((0,2))

    for u in np.arange(0,10,0.1):
        for v in np.arange(0,10,0.1):
            xn = np.array([[u, v]])
            yn = clf.predict(xn)
            if (yn<0):
                XNg = np.vstack([XNg, [u,v]])
            else:
                XNr = np.vstack([XNr, [u,v]])
        
    plotData(X,t,XNr,XNg,label)

# show results
plotResult(X,t,classifier,kernel_name)

"""# Regression data set"""

# Load the diabetes dataset
diabetes = datasets.load_diabetes()

print('Dataset: diabetes')
print('Number of features: %d' %diabetes.data.shape[1])
print('Number of samples: %d' %diabetes.data.shape[0])

# Use only one feature
diabetes_X = diabetes.data[:, np.newaxis, 2]


# Split the data into training/testing sets
diabetes_X_train = diabetes_X[:-20]
diabetes_X_test = diabetes_X[-20:]

# Split the targets into training/testing sets
diabetes_y_train = diabetes.target[:-20]
diabetes_y_test = diabetes.target[-20:]

print('Training set size: %d' %len(diabetes_X_train))
print('Test set size: %d' %len(diabetes_X_test))

"""# Regression model

Choose among different kernel functions for the SVM regressor [info](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR):
* Linear
* Polynomial
* Radial Basis Function (RBF)
* Sigmoid
"""

kernel_name  = 'poly' # 'linear', 'poly', 'rbf', 'sigmoid'

regressor = svm.SVR(C=1.0, kernel=kernel_name, degree=3, gamma='scale')

"""Fit regressor"""

regressor.fit(diabetes_X_train, diabetes_y_train)

"""# Regression prediction"""

# Commented out IPython magic to ensure Python compatibility.
# Make predictions using the testing set
diabetes_y_pred = regressor.predict(diabetes_X_test)

# The mean squared error
print("Mean squared error: %.2f"
#       % mean_squared_error(diabetes_y_test, diabetes_y_pred))

# Explained variance score: 1 is perfect prediction
print('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))

"""# Visualize results"""

# Plot outputs
plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')
plt.scatter(diabetes_X_test, diabetes_y_pred, color='red', linewidth=3)

plt.xticks(())
plt.yticks(())
plt.show()

"""# Home Exercises

**Question 1**

Test the SVM classifier with different kernel functions on the Iris and Wine datasets. Perform a grid search to provide the best configuration of the hyper-parameters.

**Question 2** 

Test the SVM regressor with different kernel functions on the full diabetes dataset. Perform a grid search to provide the best configuration of the hyper-parameters.


"""